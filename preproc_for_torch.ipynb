{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word  POS Tag\n",
       "0         1.0      Thousands  NNS   O\n",
       "1         1.0             of   IN   O\n",
       "2         1.0  demonstrators  NNS   O\n",
       "3         1.0           have  VBP   O\n",
       "4         1.0        marched  VBN   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_data/clean_data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands\n",
      "of\n",
      "demonstrators\n",
      "have\n",
      "marched\n",
      "through\n",
      "London\n",
      "to\n",
      "protest\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for r in df.values[:10]:\n",
    "    print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll first turn our dataframe to a list of lists of tuples, since this is the infinitely\n",
    "# more convenient data structure for neural nets and torch\n",
    "\n",
    "df.Word.to_list()\n",
    "\n",
    "def df_to_torch_list(df):\n",
    "    \"\"\"Function takes in dataframe with four columns:\n",
    "    Sentence #; Word; POS; Tag.\n",
    "    -------------------------------------------------\n",
    "    Returns: \n",
    "    - input_data as a list of lists (each a sentence) of tuples\n",
    "    where each tuple is (word; POS)\n",
    "    - target_data - list of lists (each a sentence) of Named \n",
    "    Entity Tags (e.g. 'O', 'B-geo', 'I-art', etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    data = df.copy()\n",
    "    for sent_ind in range(1,len(data['Sentence #'].unique().astype(int))):\n",
    "        sent_df = data.loc[data['Sentence #'] == sent_ind]\n",
    "        sent_lst = []\n",
    "        sent_target_lst = []\n",
    "        for row in sent_df.values:\n",
    "            sent_lst.append((row[1], row[2]))\n",
    "            sent_target_lst.append(row[3])\n",
    "        input_data.append(sent_lst)\n",
    "        target_data.append(sent_target_lst)\n",
    "    return input_data, target_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "input_data, target_data = df_to_torch_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Families', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('soldiers', 'NNS'),\n",
       " ('killed', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('conflict', 'NN'),\n",
       " ('joined', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('protesters', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('carried', 'VBD'),\n",
       " ('banners', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('such', 'JJ'),\n",
       " ('slogans', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('\"', '.'),\n",
       " ('Bush', 'NNP'),\n",
       " ('Number', 'NN'),\n",
       " ('One', 'CD'),\n",
       " ('Terrorist', 'NN'),\n",
       " ('\"', '.'),\n",
       " ('and', 'CC'),\n",
       " ('\"', '.'),\n",
       " ('Stop', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Bombings', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('\"', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[1]\n",
    "# target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data[2548])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know what lenght of our longest sentence is. This is important since our neural net will require all inputs to be of equal length and we'll pad shorter sentences to length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "\n",
    "Before any preprocessing occurs we will split our data into training, validation and test datasets. This might seem strange to do before creating the vocabulary, but this way, when we do get to validation and testing stage, we'll be able to see just how much 'unknown' words will impact on the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents, train_labels, test_labels = train_test_split(input_data, target_data, test_size=.2, shuffle=False)\n",
    "train_sents, valid_sents, train_labels, valid_labels = train_test_split(train_sents, train_labels, test_size=.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing into OHE\n",
    "Workflow for data preprocessing for pytorch (based on the following [resource posted by Andrew Ng and Stanford University's CS Department](https://cs230.stanford.edu/blog/namedentity/)). \n",
    "\n",
    "1. Create a unique vocabulary dict\n",
    "2. Create an NE tags dict\n",
    "3. Turning text data into lists of ints\n",
    "4. Using a batch generator to turn lists into Torch Tensors\n",
    "5. Specifying a lookup table for turning tensors to embedded arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thousands': 1,\n",
       " 'of': 106,\n",
       " 'demonstrators': 3,\n",
       " 'have': 4,\n",
       " 'marched': 56,\n",
       " 'through': 6,\n",
       " 'London': 7,\n",
       " 'to': 62,\n",
       " 'protest': 85,\n",
       " 'the': 101,\n",
       " 'war': 11,\n",
       " 'in': 100,\n",
       " 'Iraq': 13,\n",
       " 'and': 48,\n",
       " 'demand': 15,\n",
       " 'withdrawal': 17,\n",
       " 'British': 19,\n",
       " 'troops': 20,\n",
       " 'from': 57,\n",
       " 'that': 22,\n",
       " 'country': 23,\n",
       " '.': 108,\n",
       " 'UNK': 110,\n",
       " 'Families': 25,\n",
       " 'soldiers': 27,\n",
       " 'killed': 28,\n",
       " 'conflict': 31,\n",
       " 'joined': 32,\n",
       " 'protesters': 34,\n",
       " 'who': 35,\n",
       " 'carried': 36,\n",
       " 'banners': 37,\n",
       " 'with': 38,\n",
       " 'such': 39,\n",
       " 'slogans': 40,\n",
       " 'as': 41,\n",
       " '\"': 54,\n",
       " 'Bush': 43,\n",
       " 'Number': 44,\n",
       " 'One': 45,\n",
       " 'Terrorist': 46,\n",
       " 'Stop': 50,\n",
       " 'Bombings': 52,\n",
       " 'They': 55,\n",
       " 'Houses': 59,\n",
       " 'Parliament': 61,\n",
       " 'a': 63,\n",
       " 'rally': 64,\n",
       " 'Hyde': 66,\n",
       " 'Park': 67,\n",
       " 'Police': 69,\n",
       " 'put': 70,\n",
       " 'number': 72,\n",
       " 'marchers': 74,\n",
       " 'at': 75,\n",
       " '10,000': 76,\n",
       " 'while': 77,\n",
       " 'organizers': 78,\n",
       " 'claimed': 79,\n",
       " 'it': 80,\n",
       " 'was': 81,\n",
       " '1,00,000': 82,\n",
       " 'The': 84,\n",
       " 'comes': 86,\n",
       " 'on': 87,\n",
       " 'eve': 89,\n",
       " 'annual': 92,\n",
       " 'conference': 93,\n",
       " 'Britain': 95,\n",
       " \"'s\": 96,\n",
       " 'ruling': 97,\n",
       " 'Labor': 98,\n",
       " 'Party': 99,\n",
       " 'southern': 102,\n",
       " 'English': 103,\n",
       " 'seaside': 104,\n",
       " 'resort': 105,\n",
       " 'Brighton': 107}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_int_vocab(input_data : list):\n",
    "    \"\"\"Function takes in list (corpus) of lists (sentences) of dicts (words) of tuples (word, POS) and returns\n",
    "    a single vocabulary dict.\n",
    "    Returns:\n",
    "    vocab_dict - (dict) word - unique integer pairs.\"\"\"\n",
    "    vocab_dict = {}\n",
    "    i=1\n",
    "    for sentence in input_data:\n",
    "        for word_pos_tuple in sentence:\n",
    "            if word_pos_tuple not in vocab_dict.keys():\n",
    "                vocab_dict[word_pos_tuple[0]] = i\n",
    "                i +=1\n",
    "                continue\n",
    "            else: \n",
    "                continue\n",
    "        vocab_dict['UNK'] = i+1\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_test = generate_int_vocab(train_sents[:5])\n",
    "vocab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7116"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "vocabulary_dict = generate_int_vocab(train_sents)\n",
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having previously run the vocab generator on the entire dataset, the number of unique terms was 8766. Now reduced to 7115 we can gauge that at least 1.5k words will be be assigned the unknown tag across the validation and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "def generate_tag_dict(targets_list):\n",
    "    \"\"\"Function takes in a list of NE tags (which should include at least one instance of \n",
    "    every possible NE tag) and returns a dict matching each NE tag to a unique int.\n",
    "    Returns:\n",
    "    tag_map - (dict) of NE tag - associated int pairs\"\"\"\n",
    "    ne_dict = {}\n",
    "    i = 0\n",
    "    for sublist in targets_list:\n",
    "        for ne in sublist:\n",
    "            if ne in ne_dict.keys():\n",
    "                continue\n",
    "            else:\n",
    "                ne_dict[ne] = i\n",
    "                i += 1\n",
    "    return ne_dict\n",
    "\n",
    "ne_dict = generate_tag_set(train_labels)\n",
    "len(ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_ints(feature_list : list, targets_list : list, vocab : dict, ne_dict : dict, incl_POS = False, POS_dict = None):\n",
    "    \"\"\"Function takes in list (corpus) of lists (sentences) of dicts (words) of tuples (word, POS) and returns\n",
    "    a list (corpus) of lists (sentences) of integers (representing words).\n",
    "    Returns:\n",
    "    list_data.\"\"\"\n",
    "    int_sentences = []        \n",
    "    int_label_sentences = []\n",
    "    \n",
    "    for sentence in feature_list:     \n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        sentence_ints = [vocab[token[0].lower()] if token[0].lower() in vocab.keys() \n",
    "             else vocab['UNK']\n",
    "             for token in sentence]\n",
    "        int_sentences.append(sentence_ints)\n",
    "        \n",
    "    for sentence in targets_list:\n",
    "        #replace each label by its index\n",
    "        label_sent = [ne_dict[label] for label in sentence]\n",
    "        int_label_sentences.append(label_sent) \n",
    "        \n",
    "        \n",
    "    if incl_POS:\n",
    "        int_sentences_POS = []\n",
    "        for sentence in feature_list:     \n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "            sentence_POS_int = [vocab[token[1]] if token[1] in POS_dict.keys() \n",
    "                 else POS_dict['UNK_POS']\n",
    "                 for token in sentence]\n",
    "            int_sentences_POS.append(sentence_POS_int)\n",
    "        return int_sentences, int_sentences_POS, int_label_sentences\n",
    "        \n",
    "    else:     \n",
    "        return int_sentences, int_label_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# for now, we will only be generating the feature data w/out the POS tags\n",
    "train_int_sentences, train_int_label_sentences = sent_to_ints(train_sents, train_labels, vocabulary_dict, ne_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, a quick visual inspection to make sure everything has worked according to plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS'), ('of', 'IN'), ('demonstrators', 'NNS'), ('have', 'VBP')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44346\n",
      "45240\n",
      "44310\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_dict['Thousands'])\n",
    "print(vocabulary_dict['of'])\n",
    "print(vocabulary_dict['demonstrators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43858, 45240, 44310, 45232]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_int_sentences[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vects(file = 'glove/glove.6B.50d.txt', vdim=None):\n",
    "    \"\"\"Function that loads the Global representation Vectors\n",
    "    and returns them as a dictionary. \n",
    "    -----------------\n",
    "    Returns:\n",
    "    glove_dict - (dict) key - word (str), value - n-dimensional np array \"\"\"\n",
    "    glove_dict = {}\n",
    "#     total_vocab = vocab\n",
    "    if type(vdim)==int:\n",
    "        file = f'glove/glove.6B.{vdim}d.txt'\n",
    "    avg_vect = np.zeros((vdim,))\n",
    "    with open(file, 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector\n",
    "            avg_vect += vector\n",
    "        # creating the vector for new, UNKnown words in the vocabulary\n",
    "        # NOTE, this is NOT the same as the word \"unk\", which is \n",
    "        # present in glove's vocabulary\n",
    "        glove_dict['UNK'] = avg_vect/len(glove_dict)\n",
    "        glove_dict['PAD'] = np.zeros((vdim,))\n",
    "    return glove_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 24.8 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "glove_dict = load_glove_vects(vdim=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12920061, -0.28866239, -0.01224894, -0.05676689, -0.20211109,\n",
       "       -0.08389026,  0.33359737,  0.16045146,  0.03867495,  0.17833092,\n",
       "        0.0469662 , -0.00285779,  0.29099851,  0.04613723, -0.20923842,\n",
       "       -0.066131  , -0.06822448,  0.07665885,  0.31339918,  0.17848512,\n",
       "       -0.12257719, -0.09916928, -0.07495973,  0.06413206,  0.14441256,\n",
       "        0.608946  ,  0.17463101,  0.05335403, -0.01273826,  0.03474108,\n",
       "       -0.81239567, -0.04688727,  0.20193533,  0.20311115, -0.03935654,\n",
       "        0.06967518, -0.01553655, -0.03405275, -0.06528025,  0.12250092,\n",
       "        0.13992005, -0.17446305, -0.08011841,  0.08495219, -0.01041645,\n",
       "       -0.13704901,  0.20127088,  0.10069294,  0.00653007,  0.0168515 ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 'NNP'),\n",
       " ('rejects', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('criticism', 'NN'),\n",
       " (',', '.'),\n",
       " ('saying', 'VBG'),\n",
       " ('internal', 'JJ'),\n",
       " ('affairs', 'NNS'),\n",
       " ('should', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('handled', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('China', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('government', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('citizens', 'NNS'),\n",
       " (',', '.'),\n",
       " ('not', 'RB'),\n",
       " ('outsiders', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "* create input layer, matching incoming words to their respective glove_dict arrays\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('demonstrators', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('marched', 'VBN'),\n",
       " ('through', 'IN'),\n",
       " ('London', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('protest', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('war', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Iraq', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('demand', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('withdrawal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('British', 'JJ'),\n",
       " ('troops', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_dict = generate_tag_set(train_labels)\n",
    "ne_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vect(feature_list : list, targets_list : list, vocab : dict, ne_dict : dict):\n",
    "    \"\"\"Function takes in list of lists of dictionaries (input data), target NE labels,\n",
    "    a vocabulary (dictionary) and a dict of NE tags and their corresponding identifiers;\n",
    "    Returns a list of vectorised input data\"\"\"\n",
    "    vect_sentences = []        \n",
    "    vect_label_sentences = []\n",
    "    \n",
    "    for sentence in feature_list:     \n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        sentence_vectors = [vocab[token[0].lower()] if token[0].lower() in vocab.keys() \n",
    "             else vocab['UNK']\n",
    "             for token in sentence]\n",
    "        vect_sentences.append(sentence_vectors)\n",
    "        \n",
    "    for sentence in targets_list:\n",
    "        #replace each label by its index\n",
    "        try:\n",
    "            label_sent = [ne_dict[label] for label in sentence]\n",
    "        except:\n",
    "            generate_tag_set(targets_list)\n",
    "            label_sent = [ne_dict[label] for label in sentence]\n",
    "        vect_label_sentences.append(label_sent) \n",
    "        \n",
    "    return vect_sentences, vect_label_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect_sent , train_label_sent = sent_to_vect(train_sents, train_labels, glove_dict, ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_sent[200]) == len(train_vect_sent[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, )\n",
    "\n",
    "    def prep_batch(self, batch_sentences : list, batch_sentences_labels : list, vocab = self.vocab, word_vect_dim = 50):\n",
    "        \"\"\"Function takes in a list of lists (each sublist a sentence of n-dimension\n",
    "        numpy arrays), the associated list of lists of NE labels and a vocabulary (dict)\"\"\"\n",
    "        \n",
    "        #compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(sentence) for sentence in batch_sentences_labels])\n",
    "        #prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "        #and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "        #with tags from 'PAD' tokens\n",
    "        #note the dimensional change here as we are effectively about to \n",
    "        # concatenate the sentences along the 2nd dimension\n",
    "        batch_data = np.zeros((len(batch_sentences), batch_max_len, word_vect_dim))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "        #copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            #accessing individual sentence below\n",
    "            cur_len = len(batch_sentences[j])\n",
    "\n",
    "            for k in range(len(batch_sentences[j])):\n",
    "                #accessing individual word vectors below\n",
    "                batch_data[j,k, :] = batch_sentences[j][k].reshape(1,-1)\n",
    "\n",
    "            batch_labels[j][:cur_len] = batch_sentences_labels[j]\n",
    "\n",
    "        #since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.Tensor(batch_data), torch.Tensor(batch_labels)\n",
    "\n",
    "        #convert Tensors to Variables\n",
    "        # Torch tensors and torch Variables are almost the same, the latter being a wrapper fn\n",
    "        # that allows for additional methods to be called onto the underlying tensor. \n",
    "        # So we're reassigning them as Variables for extra future flexibility\n",
    "    #     batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "        yield batch_data, batch_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_for_torch as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "train_batch_1_sent, train_batch_1_labels = prep.prep_batch(train_vect_sent[:10], train_label_sent[:10], glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('demonstrators', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('marched', 'VBN'),\n",
       " ('through', 'IN'),\n",
       " ('London', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('protest', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('war', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Iraq', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('demand', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('withdrawal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('British', 'JJ'),\n",
       " ('troops', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1515e+00, -3.9703e-01,  9.7350e-01, -8.3455e-01, -1.4785e-01,\n",
       "       -4.7469e-01, -9.8629e-01,  4.4072e-01,  1.0985e-01,  7.3914e-03,\n",
       "       -4.5690e-01, -1.2794e+00,  1.0253e+00, -5.3370e-01,  1.0906e+00,\n",
       "       -3.6994e-01, -1.7323e-03, -1.2934e-02, -2.0921e-01, -8.0484e-01,\n",
       "        3.0218e-01,  2.9622e-01,  4.3949e-02, -6.2642e-02, -1.1756e-02,\n",
       "       -1.2806e+00, -2.3914e-01, -5.0524e-01,  2.8103e-01, -3.1305e-01,\n",
       "        3.0938e+00,  6.8201e-01, -3.8915e-01, -5.9624e-01, -6.8694e-01,\n",
       "        7.9195e-01, -1.5878e-01, -7.9453e-01, -2.0664e-01,  4.5275e-01,\n",
       "       -4.2613e-01,  3.5096e-01,  5.5050e-01,  2.5910e-01,  7.1832e-01,\n",
       "       -5.3633e-02, -1.0610e+00, -4.6405e-01, -9.2481e-01, -1.6236e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['thousands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1515e+00, -3.9703e-01,  9.7350e-01, -8.3455e-01, -1.4785e-01,\n",
       "        -4.7469e-01, -9.8629e-01,  4.4072e-01,  1.0985e-01,  7.3914e-03,\n",
       "        -4.5690e-01, -1.2794e+00,  1.0253e+00, -5.3370e-01,  1.0906e+00,\n",
       "        -3.6994e-01, -1.7323e-03, -1.2934e-02, -2.0921e-01, -8.0484e-01,\n",
       "         3.0218e-01,  2.9622e-01,  4.3949e-02, -6.2642e-02, -1.1756e-02,\n",
       "        -1.2806e+00, -2.3914e-01, -5.0524e-01,  2.8103e-01, -3.1305e-01,\n",
       "         3.0938e+00,  6.8201e-01, -3.8915e-01, -5.9624e-01, -6.8694e-01,\n",
       "         7.9195e-01, -1.5878e-01, -7.9453e-01, -2.0664e-01,  4.5275e-01,\n",
       "        -4.2613e-01,  3.5096e-01,  5.5050e-01,  2.5910e-01,  7.1832e-01,\n",
       "        -5.3633e-02, -1.0610e+00, -4.6405e-01, -9.2481e-01, -1.6236e+00])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_sent[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
