{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word  POS Tag\n",
       "0         1.0      Thousands  NNS   O\n",
       "1         1.0             of   IN   O\n",
       "2         1.0  demonstrators  NNS   O\n",
       "3         1.0           have  VBP   O\n",
       "4         1.0        marched  VBN   O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_data/clean_data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands\n",
      "of\n",
      "demonstrators\n",
      "have\n",
      "marched\n",
      "through\n",
      "London\n",
      "to\n",
      "protest\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for r in df.values[:10]:\n",
    "    print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll first turn our dataframe to a list of lists of tuples, since this is the infinitely\n",
    "# more convenient data structure for neural nets and torch\n",
    "\n",
    "df.Word.to_list()\n",
    "\n",
    "def df_to_torch_list(df):\n",
    "    \"\"\"Function takes in dataframe with four columns:\n",
    "    Sentence #; Word; POS; Tag.\n",
    "    -------------------------------------------------\n",
    "    Returns: \n",
    "    - input_data as a list of lists (each a sentence) of tuples\n",
    "    where each tuple is (word; POS)\n",
    "    - target_data - list of lists (each a sentence) of Named \n",
    "    Entity Tags (e.g. 'O', 'B-geo', 'I-art', etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    data = df.copy()\n",
    "    for sent_ind in range(1,len(data['Sentence #'].unique().astype(int))):\n",
    "        sent_df = data.loc[data['Sentence #'] == sent_ind]\n",
    "        sent_lst = []\n",
    "        sent_target_lst = []\n",
    "        for row in sent_df.values:\n",
    "            sent_lst.append((row[1], row[2]))\n",
    "            sent_target_lst.append(row[3])\n",
    "        input_data.append(sent_lst)\n",
    "        target_data.append(sent_target_lst)\n",
    "    return input_data, target_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "input_data, target_data = df_to_torch_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Families', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('soldiers', 'NNS'),\n",
       " ('killed', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('conflict', 'NN'),\n",
       " ('joined', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('protesters', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('carried', 'VBD'),\n",
       " ('banners', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('such', 'JJ'),\n",
       " ('slogans', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('\"', '.'),\n",
       " ('Bush', 'NNP'),\n",
       " ('Number', 'NN'),\n",
       " ('One', 'CD'),\n",
       " ('Terrorist', 'NN'),\n",
       " ('\"', '.'),\n",
       " ('and', 'CC'),\n",
       " ('\"', '.'),\n",
       " ('Stop', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Bombings', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('\"', '.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[1]\n",
    "# target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data[2548])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know what lenght of our longest sentence is. This is important since our neural net will require all inputs to be of equal length and we'll pad shorter sentences to length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vects(file = 'glove/glove.6B.50d.txt', vdim=None):\n",
    "    \"\"\"Function that loads the Global representation Vectors\n",
    "    and returns them as a dictionary. \n",
    "    -----------------\n",
    "    Returns:\n",
    "    glove_dict - (dict) key - word (str), value - n-dimensional np array \"\"\"\n",
    "    glove_dict = {}\n",
    "#     total_vocab = vocab\n",
    "    if type(vdim)==int:\n",
    "        file = f'glove/glove.6B.{vdim}d.txt'\n",
    "    avg_vect = np.zeros((vdim,))\n",
    "    with open(file, 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector\n",
    "            avg_vect += vector\n",
    "        # creating the vector for new, UNKnown words in the vocabulary\n",
    "        # NOTE, this is NOT the same as the word \"unk\", which is \n",
    "        # present in glove's vocabulary\n",
    "        glove_dict['UNK'] = avg_vect/len(glove_dict)\n",
    "        glove_dict['PAD'] = np.zeros((vdim,))\n",
    "    return glove_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 24.8 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "glove_dict = load_glove_vects(vdim=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12920061, -0.28866239, -0.01224894, -0.05676689, -0.20211109,\n",
       "       -0.08389026,  0.33359737,  0.16045146,  0.03867495,  0.17833092,\n",
       "        0.0469662 , -0.00285779,  0.29099851,  0.04613723, -0.20923842,\n",
       "       -0.066131  , -0.06822448,  0.07665885,  0.31339918,  0.17848512,\n",
       "       -0.12257719, -0.09916928, -0.07495973,  0.06413206,  0.14441256,\n",
       "        0.608946  ,  0.17463101,  0.05335403, -0.01273826,  0.03474108,\n",
       "       -0.81239567, -0.04688727,  0.20193533,  0.20311115, -0.03935654,\n",
       "        0.06967518, -0.01553655, -0.03405275, -0.06528025,  0.12250092,\n",
       "        0.13992005, -0.17446305, -0.08011841,  0.08495219, -0.01041645,\n",
       "       -0.13704901,  0.20127088,  0.10069294,  0.00653007,  0.0168515 ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents, train_labels, test_labels = train_test_split(input_data, target_data, test_size=.2, shuffle=False)\n",
    "train_sents, valid_sents, train_labels, valid_labels = train_test_split(train_sents, train_labels, test_size=.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 'NNP'),\n",
       " ('rejects', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('criticism', 'NN'),\n",
       " (',', '.'),\n",
       " ('saying', 'VBG'),\n",
       " ('internal', 'JJ'),\n",
       " ('affairs', 'NNS'),\n",
       " ('should', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('handled', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('China', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('government', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('citizens', 'NNS'),\n",
       " (',', '.'),\n",
       " ('not', 'RB'),\n",
       " ('outsiders', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "* create input layer, matching incoming words to their respective glove_dict arrays\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('demonstrators', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('marched', 'VBN'),\n",
       " ('through', 'IN'),\n",
       " ('London', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('protest', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('war', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Iraq', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('demand', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('withdrawal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('British', 'JJ'),\n",
       " ('troops', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tag_set(targets_list):\n",
    "    \"\"\"Function takes in a list of NE tags (which should include at least one instance of \n",
    "    every possible NE tag) and returns a dict matching each NE tag to a unique int.\n",
    "    Returns:\n",
    "    tag_map - (dict) of NE tag - associated int pairs\"\"\"\n",
    "    ne_dict = {}\n",
    "    i = 0\n",
    "    for sublist in targets_list:\n",
    "        for ne in sublist:\n",
    "            if ne in ne_dict.keys():\n",
    "                continue\n",
    "            else:\n",
    "                ne_dict[ne] = i\n",
    "                i += 1\n",
    "    return ne_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_dict = generate_tag_set(train_labels)\n",
    "ne_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vect(feature_list : list, targets_list : list, vocab : dict, ne_dict : dict):\n",
    "    \"\"\"Function takes in list of lists of dictionaries (input data), target NE labels,\n",
    "    a vocabulary (dictionary) and a dict of NE tags and their corresponding identifiers;\n",
    "    Returns a list of vectorised input data\"\"\"\n",
    "    vect_sentences = []        \n",
    "    vect_label_sentences = []\n",
    "    \n",
    "    for sentence in feature_list:     \n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        sentence_vectors = [vocab[token[0].lower()] if token[0].lower() in vocab.keys() \n",
    "             else vocab['UNK']\n",
    "             for token in sentence]\n",
    "        vect_sentences.append(sentence_vectors)\n",
    "        \n",
    "    for sentence in targets_list:\n",
    "        #replace each label by its index\n",
    "        try:\n",
    "            label_sent = [ne_dict[label] for label in sentence]\n",
    "        except:\n",
    "            generate_tag_set(targets_list)\n",
    "            label_sent = [ne_dict[label] for label in sentence]\n",
    "        vect_label_sentences.append(label_sent) \n",
    "        \n",
    "    return vect_sentences, vect_label_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect_sent , train_label_sent = sent_to_vect(train_sents, train_labels, glove_dict, ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_sent[200]) == len(train_vect_sent[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, )\n",
    "\n",
    "    def prep_batch(self, batch_sentences : list, batch_sentences_labels : list, vocab = self.vocab, word_vect_dim = 50):\n",
    "        \"\"\"Function takes in a list of lists (each sublist a sentence of n-dimension\n",
    "        numpy arrays), the associated list of lists of NE labels and a vocabulary (dict)\"\"\"\n",
    "        \n",
    "        #compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(sentence) for sentence in batch_sentences_labels])\n",
    "        #prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "        #and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "        #with tags from 'PAD' tokens\n",
    "        #note the dimensional change here as we are effectively about to \n",
    "        # concatenate the sentences along the 2nd dimension\n",
    "        batch_data = np.zeros((len(batch_sentences), batch_max_len, word_vect_dim))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "        #copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            #accessing individual sentence below\n",
    "            cur_len = len(batch_sentences[j])\n",
    "\n",
    "            for k in range(len(batch_sentences[j])):\n",
    "                #accessing individual word vectors below\n",
    "                batch_data[j,k, :] = batch_sentences[j][k].reshape(1,-1)\n",
    "\n",
    "            batch_labels[j][:cur_len] = batch_sentences_labels[j]\n",
    "\n",
    "        #since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.Tensor(batch_data), torch.Tensor(batch_labels)\n",
    "\n",
    "        #convert Tensors to Variables\n",
    "        # Torch tensors and torch Variables are almost the same, the latter being a wrapper fn\n",
    "        # that allows for additional methods to be called onto the underlying tensor. \n",
    "        # So we're reassigning them as Variables for extra future flexibility\n",
    "    #     batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "        yield batch_data, batch_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_for_torch as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "train_batch_1_sent, train_batch_1_labels = prep.prep_batch(train_vect_sent[:10], train_label_sent[:10], glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('demonstrators', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('marched', 'VBN'),\n",
       " ('through', 'IN'),\n",
       " ('London', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('protest', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('war', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Iraq', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('demand', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('withdrawal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('British', 'JJ'),\n",
       " ('troops', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1515e+00, -3.9703e-01,  9.7350e-01, -8.3455e-01, -1.4785e-01,\n",
       "       -4.7469e-01, -9.8629e-01,  4.4072e-01,  1.0985e-01,  7.3914e-03,\n",
       "       -4.5690e-01, -1.2794e+00,  1.0253e+00, -5.3370e-01,  1.0906e+00,\n",
       "       -3.6994e-01, -1.7323e-03, -1.2934e-02, -2.0921e-01, -8.0484e-01,\n",
       "        3.0218e-01,  2.9622e-01,  4.3949e-02, -6.2642e-02, -1.1756e-02,\n",
       "       -1.2806e+00, -2.3914e-01, -5.0524e-01,  2.8103e-01, -3.1305e-01,\n",
       "        3.0938e+00,  6.8201e-01, -3.8915e-01, -5.9624e-01, -6.8694e-01,\n",
       "        7.9195e-01, -1.5878e-01, -7.9453e-01, -2.0664e-01,  4.5275e-01,\n",
       "       -4.2613e-01,  3.5096e-01,  5.5050e-01,  2.5910e-01,  7.1832e-01,\n",
       "       -5.3633e-02, -1.0610e+00, -4.6405e-01, -9.2481e-01, -1.6236e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['thousands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1515e+00, -3.9703e-01,  9.7350e-01, -8.3455e-01, -1.4785e-01,\n",
       "        -4.7469e-01, -9.8629e-01,  4.4072e-01,  1.0985e-01,  7.3914e-03,\n",
       "        -4.5690e-01, -1.2794e+00,  1.0253e+00, -5.3370e-01,  1.0906e+00,\n",
       "        -3.6994e-01, -1.7323e-03, -1.2934e-02, -2.0921e-01, -8.0484e-01,\n",
       "         3.0218e-01,  2.9622e-01,  4.3949e-02, -6.2642e-02, -1.1756e-02,\n",
       "        -1.2806e+00, -2.3914e-01, -5.0524e-01,  2.8103e-01, -3.1305e-01,\n",
       "         3.0938e+00,  6.8201e-01, -3.8915e-01, -5.9624e-01, -6.8694e-01,\n",
       "         7.9195e-01, -1.5878e-01, -7.9453e-01, -2.0664e-01,  4.5275e-01,\n",
       "        -4.2613e-01,  3.5096e-01,  5.5050e-01,  2.5910e-01,  7.1832e-01,\n",
       "        -5.3633e-02, -1.0610e+00, -4.6405e-01, -9.2481e-01, -1.6236e+00])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_sent[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
